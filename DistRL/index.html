<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DISTRL: AN ASYNCHRONOUS DISTRIBUTED REINFORCEMENT LEARNING FRAMEWORK FOR ON-DEVICE CONTROL AGENTS</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size: 2.4rem;">DISTRL: AN ASYNCHRONOUS DISTRIBUTED REINFORCEMENT LEARNING FRAMEWORK FOR ON-DEVICE CONTROL AGENTS</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Taiyi Wang<sup>1 2 * †</sup>,
              </span>
              <span class="author-block">
                Zhihao Wu<sup>3 *</sup>,
              </span>
              <span class="author-block">
                Jianheng Liu<sup>4</sup>,
              </span>
              <span class="author-block">
                Jianye Hao<sup>3</sup>,
              </span>
              <span class="author-block">
                Jun Wang<sup>4</sup>,
              </span>
              <span class="author-block">
                Kun Shao<sup>3 †</sup>
              </span>
              </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> University of Cambridge<br><sup>2</sup> Powersense Technology Limited<br><sup>3</sup> Huawei Noah's Ark Lab</span><br><sup>4</sup> University College London</span>
                    <span class="eql-cntrb"><small><br><sup>* </sup>Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>† </sup>Corresponding authors: Taiyi.Wang@cl.cam.ac.uk, shaokun2@huawei.com</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2410.14803" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.14803" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <!-- <h2 class="title is-3 has-text-centered">Overview</h2> -->
          <div class="content">
              <h3 class="subtitle is-5">Demo:</h3>
              <video poster="" id="tree" autoplay controls muted loop height="100%">
                <!-- Your video here -->
                <!-- <source src="" type="video/mp4"> -->
              </video>
           
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            On-device control agents, especially on mobile devices, are responsible for operating mobile devices to fulfill users' requests, enabling seamless and intuitive interactions. Integrating Multimodal Large Language Models (MLLMs) into these agents enhances their ability to understand and execute complex commands, thereby improving user experience. However, fine-tuning MLLMs for on-device control presents significant challenges due to limited data availability and inefficient online training processes. This paper introduces DistRL, a novel framework designed to enhance the efficiency of online RL fine-tuning for mobile device control agents. DistRL employs centralized training and decentralized data acquisition to ensure efficient fine-tuning in the context of dynamic online interactions. Additionally, the framework is backed by our tailor-made RL algorithm, which effectively balances exploration with the prioritized utilization of collected data to ensure stable and robust training. Our experiments show that, on average, DistRL delivers a 3X improvement in training efficiency and enables training data collection 2.4X faster than the leading synchronous multi-machine methods. Notably, after training, DistRL achieves a 20% relative improvement in success rate compared to state-of-the-art methods on general Android tasks from an open benchmark, significantly outperforming existing approaches while maintaining the same training time. These results validate DistRL as a scalable and efficient solution, offering substantial improvements in both training efficiency and agent performance for real-world, in-the-wild device control tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
 
<!-- Plug-and-Play Agent Framework -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">A-RIDE: The Backbone of DistRL</h2>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <p>Our method employs advantage-based estimations to refine policy gradient updates, as an extension of Generalized Advantage Estimation (GAE) by Schulman et al. (2015), effectively balancing exploration and exploitation in the learning process.</p>
            <p>By introducing a trace decay parameter, A-RIDE manages the bias-variance trade-off in advantage calculations, optimizing the stability and convergence of the policy. A-RIDE incorporates enhancements tailored to distributed, asynchronous environments, ensuring robust policy stability and efficient learning in complex device control tasks.</p>
            <p>The <strong>Retrace(λ)</strong> update is defined as:</p>
            <p><strong>Q(s<sub>t</sub>, a<sub>t</sub>) ← Q(s<sub>t</sub>, a<sub>t</sub>) + δ<sub>t</sub></strong>, where the correction term δ<sub>t</sub> is calculated as:</p>
            <p style="text-align :center"><strong>δ<sub>t</sub> = ∑<sub>k=t</sub><sup>H</sup> γ<sup>k−t</sup> (∏<sub>i=t+1</sub><sup>k</sup> c<sub>i</sub>) [r<sub>k</sub> + γ Q(s<sub>k+1</sub>, a<sub>k+1</sub>) − Q(s<sub>k</sub>, a<sub>k</sub>)]</strong>.</p>
            <p>Here, <strong>Q(s<sub>t</sub>, a<sub>t</sub>)</strong> is the estimated action-value function; γ ∈ [0, 1] is the discount factor; H is the time horizon; c<sub>i</sub> = λ min(1, ρ<sub>i</sub>) with λ ∈ [0, 1] being the trace decay parameter; ρ<sub>i</sub> = π(a<sub>i</sub>|s<sub>i</sub>) / μ(a<sub>i</sub>|s<sub>i</sub>) is the importance sampling ratio between the target policy π and the behavior policy μ.</p>
            <p>To ensure effective exploration within the action space and prevent the generation of nonsensical or invalid commands, we incorporate entropy regularization into the actor loss function:</p>
            <p style="text-align :center"><strong>ℒ = −𝔼<sub>μ</sub>[ρ<sub>t</sub> A(s<sub>t</sub>, a<sub>t</sub>) log π(a<sub>t</sub>|s<sub>t</sub>)] − β 𝔼<sub>μ</sub>[ℍ(π(a<sub>t</sub>|s<sub>t</sub>))] + λ 𝔼<sub>μ</sub>[𝒫<sub>invalid</sub>(a<sub>t</sub>)]</strong>,</p>
            <p>where <strong>A(s<sub>t</sub>, a<sub>t</sub>)</strong> represents the advantage function, defined as the difference between the action-value function <strong>Q(s<sub>t</sub>, a<sub>t</sub>)</strong> and the state-value function <strong>V(s<sub>t</sub>)</strong>: <strong>A(s<sub>t</sub>, a<sub>t</sub>) = Q(s<sub>t</sub>, a<sub>t</sub>) − V(s<sub>t</sub>)</strong>. The entropy term ℍ promotes exploration, and 𝒫<sub>invalid</sub>(a<sub>t</sub>) imposes penalties on invalid actions. The parameters β and λ are tuned to balance exploration and stability.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Plug-and-Play Agent Framework -->


<!-- Plug-and-Play Agent Framework -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">DistRL Pipeline</h2>
      <div class="columns is-centered">         
        <div class="column is-four-fifths">
          <div class="content">
            <p><strong>DistRL</strong> adopts a distributed asynchronous setup where multiple worker agents generate trajectories under the behavior policy <strong>μ</strong> and send them to a central learner.</p>
            <p>The trajectory reward is computed using the Monte Carlo estimate:</p>
            <p style="text-align :center"><strong>L(V<sub>traj</sub>) = −𝔼<sub>ν</sub>[r(s<sub>H</sub>, a<sub>H</sub>) log V<sub>traj</sub>(s<sub>H</sub>, a<sub>H</sub>) + (1 − r(s<sub>H</sub>, a<sub>H</sub>)) log (1 − V<sub>traj</sub>(s<sub>H</sub>, a<sub>H</sub>))]</strong>.</p>
            <p>The actor is updated using policy gradients based on advantage estimates, and enhanced <strong>Retrace</strong> corrections are applied for off-policy learning. This process is distributed asynchronously across worker nodes, ensuring efficient fine-tuning in environments with sparse rewards and distributed delays.</p>
          </div>
        <img src="static/images/backbone.PNG" width="100%">
      </div>
    </div>
  </div>
</section>
<!-- End Plug-and-Play Agent Framework -->
 

<!-- Results -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title has-text-centered">Results</h2>
      <div class="columns is-centered">
        <div class="column is-four-fifths">


  <table id="results-table" border="1" cellpadding="5" cellspacing="0" style="width:100%; border-collapse:collapse;">
    <thead>
      <tr>
          <th rowspan="2" style="padding:5px;">Framework Type</th>
          <th rowspan="2" style="padding:5px;">Framework Name</th>
          <th colspan="2" style="padding:5px;">General</th>
          <th colspan="2" style="padding:5px;">Web Shopping</th>
      </tr>
      <tr>
          <th style="padding:5px;">Training</th>
          <th style="padding:5px;">Test</th>
          <th style="padding:5px;">Training</th>
          <th style="padding:5px;">Test</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td rowspan="2" style="padding:5px; vertical-align: middle;">Prompting</td>
          <td style="padding:5px;text-align: left">AppAgent + GPT-4v</td>
          <td style="padding:5px;">41.4</td>
          <td style="padding:5px;">43.0</td>
          <td style="padding:5px;">31.2</td>
          <td style="padding:5px;">35.2</td>
      </tr>
      <tr>
          <td style="padding:5px;">AppAgent + Gemini</td>
          <td style="padding:5px;">39.1</td>
          <td style="padding:5px;">45.3</td>
          <td style="padding:5px;">30.5</td>
          <td style="padding:5px;">32.0</td>
      </tr>
      <tr>
          <td rowspan="4" style="padding:5px;vertical-align: middle">Learning</td>
          <td style="padding:5px; text-align: left">AutoUI</td>
          <td style="padding:5px;">38.3</td>
          <td style="padding:5px;">40.6</td>
          <td style="padding:5px;">42.2</td>
          <td style="padding:5px;">44.5</td>
      </tr>
      <tr>
          <td style="padding:5px;">DigiRL (single, online)</td>
          <td style="padding:5px;">64.6 ± 1.5</td>
          <td style="padding:5px;">59.9 ± 2.1</td>
          <td style="padding:5px;">63.3 ± 1.5</td>
          <td style="padding:5px;">59.6 ± 3.1</td>
      </tr>
      <tr>
          <td style="padding:5px;">DigiRL (multi)</td>
          <td style="padding:5px;">67.7 ± 1.3</td>
          <td style="padding:5px;">61.2 ± 2.4</td>
          <td style="padding:5px;">64.5 ± 1.1</td>
          <td style="padding:5px;">59.9 ± 2.8</td>
      </tr>
      <tr>
          <td style="padding:5px; font-weight:bold; font-style:italic;">DistRL (Ours)</td>
          <td style="padding:5px; font-weight:bold;">75.5 ± 0.2</td>
          <td style="padding:5px; font-weight:bold;">73.2 ± 1.1</td>
          <td style="padding:5px; font-weight:bold;">69.8 ± 0.5</td>
          <td style="padding:5px; font-weight:bold;">68.5 ± 1.7</td>
      </tr>
  </tbody>
</table>
<h2 class="subtitle has-text-centered" style="font-size: 1rem; margin-top: 1rem;">
  Main comparisons regarding the success rate of different agents across various settings. Each experiment is repeated three times and the mean and standard deviation are reported. Results are evaluated with our autonomous evaluator with the 128 user instructions in the train and test set.
</h2>


  <div class="content">
    <h3 class="subtitle is-6" style="margin-top: 1rem;">For full results and more details, please refer to our <a href="https://arxiv.org/abs/2410.14803" target="_blank" style="color:#3273dc;">paper</a>.</h3>
</div>
</div>
</div>
</div>
</div>
</section>
<!--End Results -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{wang2024distrl,
  title={DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents},
  author={Wang, Taiyi and Wu, Zhihao and Liu, Jianheng and Hao, Jianye and Wang, Jun and Shao, Kun},
  journal={arXiv preprint arXiv:2410.14803},
  year={2024}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

<style>
  #results-table{
    background-color: #FFFFFF;
  }
  #results-table > thead > tr > th {
    padding: 0.5rem;
    text-align: center;
    vertical-align: middle;
  }
  #results-table > tbody > tr > td {
    padding: 0.5rem;
  }
  #results-table > tbody > tr > td:nth-child(n+2) {
    text-align: center;
  }
</style>